HiresW v8.0.1 - re-released November 18, 2020
HiresW v8.0.1 - released October 5, 2020


Google docs version (cleaner formatting) at: 


https://docs.google.com/document/d/158iwNB4Shz0rp4Bg9D9WDD3gC6HT_bvotohUN319Bm0/edit?usp=sharing




#####################
Update overview 
#####################


* Includes changes for the FV3 to initialize from GFSv16 (changing from nemsio to netcdf files).  This version was built on top of the Nov 15 release of HiresW v8.0.0


#####################
Obtaining and building the package
#####################


Clone it with this command (but change hiresw.v8.0.1 to whatever you want to call it in the directory that it is cloned into):


git clone -b v8.0.1_beta  https://github.com/MatthewPyle-NOAA/regional_workflow.git hiresw.v8.0.1




Simplified installation directions:


1. Build the executables


cd sorc
./build_all.sh
Within this overall “build_all” script there are steps to check out external codes (for the FV3), and a copying in of fix files for both ARW and FV3.


The actual build work is being done by the arw/build_hiresw.sh script and fv3/build_all.sh scripts.  
 The BUILD_* variables near the top of the ARW build_hiresw.sh script allow for selecting which codes to build.   The regional_build.cfg file in sorc/fv3 allows for specifying which components are compiled for the FV3 build.
2. Install the executables for both ARW and FV3
./install_all.sh
Again, the actual work is done by model specific scripts, in this case arw/install_hiresw.sh script and fv3/install_all.sh.
The arw/install_hiresw.sh script has INSTALL_* variables corresponding to the BUILD_* variables in arw/build_hiresw.sh in case there is a desire to only copy select executables to the final executable space.  Executing the script copies the executables to the proper exec/arw/ space.
#############################
Job/script changes of note:
#############################




The JHIRESW_MAKE_BC job is reworked to generate all boundary information in two sequential chunks to prevent an effective doubling of node resources for this job with GFSv16 files, so will run up to 4 minutes longer than the original version for GFSv15.


JHIRESW_MAKE_BC (20 nodes/80 tasks for HI/PR/Guam; 40 nodes/160 tasks for CONUS and AK)




===============================================


Remaining details follow the release notes for HiresW v8.0.0




#####################
Product changes 
#####################




GRIB output changes:


In all comparisons here, CC is the cycle time, and FF is the forecast hour.  The right side of the --> indicates the file name in the new system.






1. For the main 5 km output grids:


hiresw.tCCz.nmmb_5km.fFF.DOM.grib2 --> hiresw.tCCz.fv3_5km.fFF.DOM.grib2


Where DOM is domain  (conus | guam | hi | pr | guam)


The new FV3 files have these products not in NMMB:


WEASD:surface:FF hour fcst: (instantaneous WEASD)
APCP:surface:0-FF hour acc fcst:(running total APCP)


FV3 also has consistent average heat flux computed over the previous hour - the NMMB averages fluxes with a 3 h bucket, so the period covered is variable (f01 would average f00-f01, f02 would average f00-f02, f03 would average f00-f03, f04 would average f03-f04)


2. For the 2p5km and 3km NDFD grids:


hiresw.tCCz.nmmb_2p5km.fFF.DOM.grib2 --> hiresw.tCCz.fv3_2p5km.fFF.DOM.grib2      
Where DOM is domain  (conus | guam | hi | pr)


+
hiresw.tCCz.nmmb_3km.fFF.ak.grib2 --> hiresw.tCCz.fv3_3km.fFF.ak.grib2      




The new FV3 files have these products not in NMMB:


WEASD:surface:FF hour fcst: (instantaneous WEASD)
APCP:surface:0-FF hour acc fcst:(running total APCP)






3. The CONUS output for AWIPS/SBN distribution is changed from east CONUS and west CONUS grids into a single CONUS grid.


hiresw.tCCz.nmmb_5km.fFF.conus{east|west}.grib2 --> hiresw.tCCz.fv3_5km.fFF.conussbn.grib2


The new FV3 files have these products not in NMMB:


WEASD:surface:FF hour fcst: (instantaneous WEASD)
APCP:surface:0-FF hour acc fcst:(running total APCP)


FV3 also has consistent average heat flux computed over the previous hour - the NMMB averages fluxes with a 3 h bucket, so the period covered is variable (f01 would average f00-f01, f02 would average f00-f02, f03 would average f00-f03, f04 would average f03-f04)


The same change to a single output grid for CONUS SBN also  is made for the CONUS ARW output:


hiresw.tCCz.arw_5km.fFF.conus{east|west}.grib2 --> hiresw.tCCz.arw_5km.fFF.conussbn.grib2




4. The CONUS and Alaska “subset” grids have the following changes:


hiresw.t??z.nmmb_3km.fFF.conus.subset.grib2 --> hiresw.t??z.fv3_3km.fFF.conus.subset.grib2


hiresw.t??z.nmmb_5km.fFF.ak.subset.grib2 --> hiresw.t??z.fv3_5km.fFF.ak.subset.grib2


The FV3 files have these products not in NMMB:


WEASD:surface:FF hour fcst: (instantaneous WEASD)
APCP:surface:0-FF hour acc fcst:(running total APCP)






5. The 00 h output in both the ARW and FV3 model output has been cleaned up by eliminating hourly maximum and minimum fields, average fields, and accumulation fields, all of which lack meaning at the 00 h forecast time.


1. For the 00 h main 5 km output grids:


hiresw.tCCz.nmmb_5km.f00.DOM.grib2 --> hiresw.tCCz.fv3_5km.f00.DOM.grib2
+
hiresw.tCCz.arw_5km.f00.DOM.grib


Where DOM is domain  (conus | guam | hi | pr | guam), these records are removed:


MAXUVV:100-1000 mb::
MAXDVV:100-1000 mb::
MAXUW:10 m above ground::
MAXVW:10 m above ground::
MAXREF:1000 m above ground::
MXUPHL:5000-2000 m above ground::
MXUPHL:3000-0 m above ground::
REFD:263 K level:0-0 day max fcst:
MNUPHL:5000-2000 m above ground::
MNUPHL:3000-0 m above ground::
APCP:surface:0-0 day acc fcst
WEASD:surface:0-0 day acc fcst:
TMAX:2 m above ground::
MAXRH:2 m above ground::
TMIN:2 m above ground::
MINRH:2 m above ground::


These records are only removed by shifting from NMMB to FV3 output (no such records exist in ARW output) 


SHTFL:surface:0-0 day ave fcst:
LHTFL:surface:0-0 day ave fcst:




2. For the 2p5km and 3km NDFD grid output:


hiresw.tCCz.nmmb_2p5km.f00.DOM.grib2 --> hiresw.tCCz.fv3_2p5km.f00.DOM.grib2      
hiresw.tCCz.nmmb_3km.f00.ak.grib2 --> hiresw.tCCz.fv3_3km.f00.ak.grib2      
+
hiresw.tCCz.arw_2p5km.f00.DOM.grib2
hiresw.tCCz.arw_3km.f00.ak.grib2


Where DOM is domain  (conus/guam/hi/pr), these records are removed:


MAXUVV:100-1000 mb::
MAXREF:1000 m above ground::
MXUPHL:5000-2000 m above ground::
TMAX:2 m above ground::
TMIN:2 m above ground::
MAXRH:2 m above ground::
MINRH:2 m above ground::
MAXUW:10 m above ground::
MAXVW:10 m above ground::


3. For the 00 h “subset” product grids for CONUS and Alaska:


hiresw.t??z.nmmb_3km.f00.conus.subset.grib2 --> hiresw.t??z.fv3_3km.f00.conus.subset.grib2


hiresw.t??z.nmmb_5km.f00.ak.subset.grib2 --> hiresw.t??z.fv3_5km.f00.ak.subset.grib2


+
hiresw.t??z.arw_3km.f00.conus.subset.grib2
hiresw.t??z.arw_5km.f00.ak.subset.grib2




These records are removed:


MAXUVV:100-1000 mb::
MAXDVV:100-1000 mb::
MAXUW:10 m above ground::
MAXVW:10 m above ground::
MAXREF:1000 m above ground::
MXUPHL:5000-2000 m above ground::
MXUPHL:3000-0 m above ground::
REFD:263 K level:0-0 day max fcst:
MNUPHL:5000-2000 m above ground::
MNUPHL:3000-0 m above ground::
APCP:surface:0-0 day acc fcst
WEASD:surface:0-0 day acc fcst:


4. For the 00 h CONUS files specifically created for SBN:


Currently hiresw.tCCz.*_5km.f00.conus{east|west}.grib2
Becoming hiresw.tCCz.*_5km.f00.conussbn.grib2


This record is removed:


REFD:263 K level:0-0 day max fcst:




BUFR output changes:


In comparisons here, CC is the cycle time:


File names are modified:


hiresw.tCCz.conusnmmb.class1.bufr --> hiresw.tCCz.conusfv3.class1.bufr
hiresw.tCCz.conusnmmb.class1.bufr.wcoss --> hiresw.tCCz.conusfv3.class1.bufr.wcoss


53 stations that existed in CONUS NMMB BUFR output are eliminated in CONUS FV3 BUFR output due to a smaller integration domain.  The associated station identifiers and station numbers for the stations eliminated are listed in the below table:




 
  CHRL             14
  CNLK             16
   WJA             17
  MRYS            256
   S#1            310
   NW4            333
   NW5            334
   NW6            335
   NW9            338
   B#G          46006
   G#A          90010
   G#D          90013
   G#G          90016
  CWZV         710310
  CYPE         710680
  CYYL         710780
  CYTH         710790


	 
  CYZT         711090
  CWSE         711190
  CYEG         711230
  CYLJ         711250
  CYBU         711300
  CWZB         711970
  CWSA         716000
  CYQY         717070
  CYQX         718030
  CYDF         718090
  CYZV         718110
  CYJT         718150
  CYAH         718230
  CYTL         718480
  CYFO         718575
  CYQD         718670
  CYXS         718960
 


	

  CWLB         719310
  MMZC         765255
  TXKF         780160
  MUNG         782210
  MUCM         782550
  MUMZ         782560
  MUBY         782593
  MUCU         782640
  MUGT         782670
  MUBA         782680
  MUMO         782684
  MUCL         783334
  MUVT         783570
  MUGM         783670
  MTCH         784090
  MDPP         784570
  MDST         784600
  MDSD         784850
  DSD          784860


	



hiresw.tCCz.aknmmb.class1.bufr --> hiresw.tCCz.akfv3.class1.bufr


For the Alaska domain, a single station that was in AK NMMB BUFR output is eliminated in the AK FV3 BUFR output due to a smaller integration domain.   The associated station identifier and station number for the station eliminated is listed in the below table




 NW8        337


	

hiresw.tCCz.hinmmb.class1.bufr --> hiresw.tCCz.hifv3.class1.bufr
hiresw.tCCz.prnmmb.class1.bufr --> hiresw.tCCz.prfv3.class1.bufr
hiresw.tCCz.guamnmmb.class1.bufr --> hiresw.tCCz.guamfv3.class1.bufr


The stations provided by FV3 BUFR output for Hawaii, Puerto Rico, and Guam match the list of stations previously provided by the NMMB BUFR output.




#############################
Job/script changes of note:
#############################


Eliminates jobs used only by the HiresW-NMMB system that is being removed.


JHIRESW_NEMSINTERP_1
JHIRESW_NEMSINTERP_2
JHIRESW_NEMSINTERP_3
JHIRESW_NEMSINTERP_4


New FV3-only preprocessing jobs are added to generate the model initial conditions and lateral boundary conditions, respectively.  The JHIRESW_MAKE_BC job is designed to generate all boundary information simultaneously, so it is computationally intense but completes in about 4-7 minutes depending on domain.  


JHIRESW_MAKE_IC (4 nodes, 16 tasks total)
JHIRESW_MAKE_BC (20 nodes/80 tasks for HI/PR/Guam; 40 nodes/160 tasks for CONUS and AK)


The FV3 uses JHIRESW_POST_ODD and JHIRESW_POST_EVEN for all domains (the NMMB and ARW only use them for the CONUS domain) .  Snow depth changes for FV3 are computed by differencing the instantaneous snow depth at successive hours, so the odd/even hour jobs have a bit of overlap for the FV3 system.  The JHIRESW_POST_EVEN job (and scripts/fv3/exhiresw_post_even.sh.ecf) handles all of this differencing.  Both jobs need to be run to generate complete forecast output at a given hour. 


A new odd/even hour PRDGEN job is added (only used by the FV3 for the Alaska and CONUS domains):


JHIRESW_PRDGEN_EVEN
JHIRESW_PRDGEN_ODD


Resource usage for the POST and PRDGEN jobs are listed in the resource changes section below.


The rocoto/hiresw_*.xml files show how the parallel test system launched jobs and assigned resources.  Many of the resource definitions come from within rocoto/sites/wcoss_cray.ent


#############################
Resource changes for forecast job
f#############################


CONUS


Prod NMMB ~  3720 seconds on 41 nodes
Para FV3  ~ 5373 seconds on 93 nodes  (30 x 72 + 3 x 24)
Para FV3 ~7210 seconds on 151 nodes (32 x 108 + 7 x 24) [ I/O hurt performance ]


AK 


Prod NMMB ~  4020 seconds on 27 nodes
Para FV3  ~ 5638 seconds on 51 nodes (24 x 48 + 3 x 24)
Para FV3  ~ 4618 seconds on 102 nodes (24 x 96 + 6 x 24) [ I/O limits performance]


PR 


Prod NMMB ~  3467 seconds on 5 nodes
Para FV3  ~ 4121 seconds on 10 nodes 


HI 


Prod NMMB ~  3387 seconds on 2 nodes
Para FV3  ~ 3972 seconds on 7 nodes 


GUAM:


Prod NMMB ~  3417 seconds on 3 nodes
Para FV3  ~ 3815 seconds on 7 nodes 




#############################
Resource changes for post and prdgen jobs (combined)
#############################


CONUS


Prod NMMB ~  (4+4 for post, 1 for prdgen) = 9 nodes total
Para FV3  ~ (3+3 for post, 1+1 for prdgen) = 8 nodes total


AK 


Prod NMMB ~  (3 for post, 1 for prdgen) = 4 nodes total
Para FV3  ~ (2+2 for post, 1+1 for prdgen) = 6 nodes total


PR 


Prod NMMB ~  (1 for post, 1 for prdgen) = 2 nodes total
Para FV3  ~ (1+1 for post, 1 for prdgen) = 3 nodes total


HI 


Prod NMMB ~  (1 for post, 1 for prdgen) = 2 nodes total
Para FV3  ~ (1+1 for post, 1 for prdgen) = 3 nodes total




GUAM:


Prod NMMB ~  (1 for post, 1 for prdgen) = 2 nodes total
Para FV3  ~ (1+1 for post, 1 for prdgen) = 3 nodes total


#############################
Resource changes for bufrpost jobs 
#############################


CONUS:


Prod NMMB ~ (2 x (1 task/1 node)) = 2 nodes
Para FV3 ~ (24 tasks/3 nodes) = 3 nodes


AK:


Prod NMMB ~ (1 task/1 node) = 1 node
Para FV3 ~ (8 tasks/1 node) = 1 node


PR, HI, Guam:


Prod NMMB ~ (1 task/1 node) = 1 node
Para FV3 ~ (1 tasks/1 node) = 1 node




#############################
DISK SPACE CHANGES - 
#############################


ops com/hiresw/prod/ (just NMMB aspect)


341.9 GB hiresw.${PDY} + NMMB awips and nawips aspects not currently under hiresw/prod/


para com/hiresw/prod/ (just FV3 aspect)


556.1 GB hiresw.${PDY}
* which includes awips and nawips pieces now going under $COMOUT, and inputs stored under nwges






####################
## Archiving to HPSS
####################


Some limited changes to how HIRESW data is archived are being proposed.  The first is to archive by cycle time rather than one single archive for the day.  The second critical item is to replace any “nmmb” references with “fv3” due to the model change within the HiresW.  Although the FV3 runs to 60 h, only the first 48 hours will be archived (just swapping nmmb with fv3 in what is currently saved)..


Comparing the size of just the NMMB aspects going to HPSS against the potential FV3 aspects, it looks like a very slight increase (~19.4 GB/day to ~20.0 GB/day) with this upgrade.  An HPCRAC request has been drafted but hasn’t been submitted as of this writing.
